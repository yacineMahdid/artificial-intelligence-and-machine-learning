{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layers Perceptron\n",
    "The goal of this notebook is to showcase how to code a multilayer perceptron in Python from scratch.\n",
    "It makes use of the Perceptron algorithm we developped in the perceptron jupyter notebook with modification to use backpropagation!\n",
    "\n",
    "We have two main class:\n",
    "- Neuron: Is used to model one neuron, most of the computation happens there\n",
    "- MultiLayerPerceptron: Is used to model the full neural network. Here we only support fully connected neural network.\n",
    "- In this notebook we will code it in such a way that we can have a variable architecture.\n",
    "\n",
    "The one limitation here is that we still only have one output that is possible 1 or 0. We still didn't got around to have arbitrary amount of output (getting there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class Neuron():\n",
    "    '''\n",
    "        A conceptual Neuron hat can be trained using a \n",
    "        fit and predict methodology, without any library\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, position_in_layer, is_output_neuron=False):\n",
    "        self.weights = []\n",
    "        self.inputs = []\n",
    "        self.output = None\n",
    "        \n",
    "        # This is used for the backpropagation update\n",
    "        self.updated_weights = []\n",
    "        # This is used to know how to update the weights\n",
    "        self.is_output_neuron = is_output_neuron\n",
    "        # This delta is used for the update at the backpropagation\n",
    "        self.delta = None\n",
    "        # This is used for the backpropagation update\n",
    "        self.position_in_layer = position_in_layer \n",
    "        \n",
    "    def attach_to_output(self, neurons):\n",
    "        '''\n",
    "            Helper function to store the reference of the other neurons\n",
    "            To this particular neuron (used for backpropagation)\n",
    "        '''\n",
    "        \n",
    "        self.output_neurons = neurons\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "            simple sigmoid function (logistic) used for the activation\n",
    "        '''\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    def init_weights(self, num_input):\n",
    "        '''\n",
    "            This is used to setup the weights when we know how many inputs there is for\n",
    "            a given neuron\n",
    "        '''\n",
    "        \n",
    "        # Randomly initalize the weights\n",
    "        for i in range(num_input+1):\n",
    "            self.weights.append(random.uniform(0,1))\n",
    "        \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            Given a row of data it will predict what the output should be for\n",
    "            this given neuron. We can have many input, but only one output for a neuron\n",
    "        '''\n",
    "        \n",
    "        # Reset the inputs\n",
    "        self.inputs = []\n",
    "        \n",
    "        # We iterate over the weights and the features in the given row\n",
    "        activation = 0\n",
    "        for weight, feature in zip(self.weights, row):\n",
    "            self.inputs.append(feature)\n",
    "            activation = activation + weight*feature\n",
    "            \n",
    "        \n",
    "        self.output = self.sigmoid(activation)\n",
    "        return self.output\n",
    "    \n",
    "        \n",
    "            \n",
    "    def update_neuron(self):\n",
    "        '''\n",
    "            Will update a given neuron weights by replacing the current weights\n",
    "            with those used during the backpropagation. This need to be done at the end of the\n",
    "            backpropagation\n",
    "        '''\n",
    "        \n",
    "        self.weights = []\n",
    "        for new_weight in self.updated_weights:\n",
    "            self.weights.append(new_weight)\n",
    "    \n",
    "    def calculate_update(self, learning_rate, target):\n",
    "        '''\n",
    "            This function will calculate the updated weights for this neuron. It will first calculate\n",
    "            the right delta (depending if this neuron is a ouput or a hidden neuron), then it will\n",
    "            calculate the right updated_weights. It will not overwrite the weights yet as they are needed\n",
    "            for other update in the backpropagation algorithm.\n",
    "        '''\n",
    "        \n",
    "        if self.is_output_neuron:\n",
    "            # Calculate the delta for the output\n",
    "            self.delta = (self.output - target)*self.output*(1-self.output)\n",
    "        else:\n",
    "            # Calculate the delta\n",
    "            delta_sum = 0\n",
    "            # this is to know which weights this neuron is contributing in the output layer\n",
    "            cur_weight_index = self.position_in_layer \n",
    "            for output_neuron in self.output_neurons:\n",
    "                delta_sum = delta_sum + (output_neuron.delta * output_neuron.weights[cur_weight_index])\n",
    "\n",
    "            # Update this neuron delta\n",
    "            self.delta = delta_sum*self.output*(1-self.output)\n",
    "            \n",
    "            \n",
    "        # Reset the update weights\n",
    "        self.updated_weights = []\n",
    "        \n",
    "        # Iterate over each weight and update them\n",
    "        for cur_weight, cur_input in zip(self.weights, self.inputs):\n",
    "            gradient = self.delta*cur_input\n",
    "            new_weight = cur_weight - learning_rate*gradient\n",
    "            self.updated_weights.append(new_weight)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    '''\n",
    "        Layer is modelizing a layer in the fully-connected-feedforward neural network architecture.\n",
    "        It will play the role of connecting everything together inside and will be doing the backpropagation \n",
    "        update.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_neuron, is_output_layer = False):\n",
    "        \n",
    "        # Will create that much neurons in this layer\n",
    "        self.is_output_layer = is_output_layer\n",
    "        self.neurons = []\n",
    "        for i in range(num_neuron):\n",
    "            # Create neuron\n",
    "            neuron = Neuron(i,  is_output_neuron=is_output_layer)\n",
    "            self.neurons.append(neuron)\n",
    "    \n",
    "    def attach(self, layer):\n",
    "        '''\n",
    "            This function attach the neurons from this layer to another one\n",
    "            This is needed for the backpropagation algorithm\n",
    "        '''\n",
    "        # Iterate over the neurons in the current layer and attach \n",
    "        # them to the next layer\n",
    "        for in_neuron in self.neurons:\n",
    "            in_neuron.attach_to_output(layer.neurons)\n",
    "            \n",
    "    def init_layer(self, num_input):\n",
    "        '''\n",
    "            This will initialize the weights of each neuron in the layer.\n",
    "            By giving the right num_input it will spawn the right number of weights\n",
    "        '''\n",
    "        \n",
    "        # Iterate over each of the neuron and initialize\n",
    "        # the weights that connect with the previous layer\n",
    "        for neuron in self.neurons:\n",
    "            neuron.init_weights(num_input)\n",
    "    \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            This will calcualte the activations for the full layer given the row of data \n",
    "            streaming in.\n",
    "        '''\n",
    "        row.append(1) # need to add the bias\n",
    "        activations = [neuron.predict(row) for neuron in self.neurons]\n",
    "        return activations\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    '''\n",
    "        We will be creating the multi-layer perceptron with only two layer:\n",
    "        an input layer, a perceptrons layer and a one neuron output layer which does binary classification\n",
    "    '''\n",
    "    def __init__(self, learning_rate = 0.01, num_iteration = 100):\n",
    "        \n",
    "        # Layers\n",
    "        self.layers = []\n",
    "                \n",
    "        # Training parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iteration = num_iteration\n",
    "        \n",
    "        \n",
    "    def add_output_layer(self, num_neuron):\n",
    "        '''\n",
    "            This helper function will create a new output layer and add it to the architecture\n",
    "        '''\n",
    "        self.layers.insert(0, Layer(num_neuron, is_output_layer = True))\n",
    "    \n",
    "    def add_hidden_layer(self, num_neuron):\n",
    "        '''\n",
    "            This helper function will create a new hidden layer, add it to the architecture\n",
    "            and finally attach it to the front of the architecture\n",
    "        '''\n",
    "        # Create an hidden layer\n",
    "        hidden_layer = Layer(num_neuron)\n",
    "        # Attach the last added layer to this new layer\n",
    "        hidden_layer.attach(self.layers[0])\n",
    "        # Add this layers to the architecture\n",
    "        self.layers.insert(0, hidden_layer)\n",
    "        \n",
    "    def update_layers(self, target):\n",
    "        '''\n",
    "            Will update all the layers by calculating the updated weights and then updating \n",
    "            the weights all at once when the new weights are found.\n",
    "        '''\n",
    "        # Iterate over each of the layer in reverse order\n",
    "        # to calculate the updated weights\n",
    "        for layer in reversed(self.layers):\n",
    "                           \n",
    "            # Calculate update the hidden layer\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.calculate_update(self.learning_rate, target)  \n",
    "        \n",
    "        # Iterate over each of the layer in normal order\n",
    "        # to update the weights\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.update_neuron()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "            Main training function of the neural network algorithm. This will make use of backpropagation.\n",
    "            It will use stochastic gradient descent by selecting one row at random from the dataset and \n",
    "            use predict to calculate the error. The error will then be backpropagated and new weights calculated.\n",
    "            Once all the new weights are calculated, the whole network weights will be updated\n",
    "        '''\n",
    "        num_row = len(X)\n",
    "        num_feature = len(X[0]) # Here we assume that we have a rectangular matrix\n",
    "        \n",
    "        # Init the weights throughout each of the layer\n",
    "        self.layers[0].init_layer(num_feature)\n",
    "        \n",
    "        for i in range(1, len(self.layers)):\n",
    "            num_input = len(self.layers[i-1].neurons)\n",
    "            self.layers[i].init_layer(num_input)\n",
    "\n",
    "        # Launch the training algorithm\n",
    "        for i in range(self.num_iteration):\n",
    "            \n",
    "            # Stochastic Gradient Descent\n",
    "            r_i = random.randint(0,num_row-1)\n",
    "            row = X[r_i] # take the random sample from the dataset\n",
    "            yhat = self.predict(row)\n",
    "            target = y[r_i]\n",
    "            \n",
    "            # Update the layers using backpropagation   \n",
    "            self.update_layers(target)\n",
    "            \n",
    "            # At every 100 iteration we calculate the error\n",
    "            # on the whole training set\n",
    "            if i % 1000 == 0:\n",
    "                total_error = 0\n",
    "                for r_i in range(num_row):\n",
    "                    row = X[r_i]\n",
    "                    yhat = self.predict(row)\n",
    "                    error = (y[r_i] - yhat)\n",
    "                    total_error = total_error + error**2\n",
    "                mean_error = total_error/num_row\n",
    "                print(f\"Iteration {i} with error = {mean_error}\")\n",
    "        \n",
    "    \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            Prediction function that will take a row of input and give back the output\n",
    "            of the whole neural network.\n",
    "        '''\n",
    "        \n",
    "        # Gather all the activation in the hidden layer\n",
    "        \n",
    "        activations = self.layers[0].predict(row)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            activations = self.layers[i].predict(activations)\n",
    "\n",
    "        outputs = []\n",
    "        for activation in activations:                        \n",
    "            # Decide if we output a 1 or 0\n",
    "            if activation >= 0.5:\n",
    "                outputs.append(1.0)\n",
    "            else:\n",
    "                outputs.append(0.0)\n",
    "                           \n",
    "        # We currently have only One output allowed\n",
    "        return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 with error = 0.5\n",
      "Iteration 1000 with error = 0.5\n",
      "Iteration 2000 with error = 0.5\n",
      "Iteration 3000 with error = 0.5\n",
      "Iteration 4000 with error = 0.25\n",
      "Iteration 5000 with error = 0.5\n",
      "Iteration 6000 with error = 0.5\n",
      "Iteration 7000 with error = 0.5\n",
      "Iteration 8000 with error = 0.5\n",
      "Iteration 9000 with error = 0.5\n",
      "Iteration 10000 with error = 0.75\n",
      "Iteration 11000 with error = 0.75\n",
      "Iteration 12000 with error = 0.5\n",
      "Iteration 13000 with error = 0.25\n",
      "Iteration 14000 with error = 0.25\n",
      "Iteration 15000 with error = 0.25\n",
      "Iteration 16000 with error = 0.25\n",
      "Iteration 17000 with error = 0.25\n",
      "Iteration 18000 with error = 0.25\n",
      "Iteration 19000 with error = 0.25\n",
      "Iteration 20000 with error = 0.0\n",
      "Iteration 21000 with error = 0.0\n",
      "Iteration 22000 with error = 0.0\n",
      "Iteration 23000 with error = 0.0\n",
      "Iteration 24000 with error = 0.0\n",
      "Iteration 25000 with error = 0.0\n",
      "Iteration 26000 with error = 0.0\n",
      "Iteration 27000 with error = 0.0\n",
      "Iteration 28000 with error = 0.0\n",
      "Iteration 29000 with error = 0.0\n",
      "Iteration 30000 with error = 0.0\n",
      "Iteration 31000 with error = 0.0\n",
      "Iteration 32000 with error = 0.0\n",
      "Iteration 33000 with error = 0.0\n",
      "Iteration 34000 with error = 0.0\n",
      "Iteration 35000 with error = 0.0\n",
      "Iteration 36000 with error = 0.0\n",
      "Iteration 37000 with error = 0.0\n",
      "Iteration 38000 with error = 0.0\n",
      "Iteration 39000 with error = 0.0\n",
      "Iteration 40000 with error = 0.0\n",
      "Iteration 41000 with error = 0.0\n",
      "Iteration 42000 with error = 0.0\n",
      "Iteration 43000 with error = 0.0\n",
      "Iteration 44000 with error = 0.0\n",
      "Iteration 45000 with error = 0.0\n",
      "Iteration 46000 with error = 0.0\n",
      "Iteration 47000 with error = 0.0\n",
      "Iteration 48000 with error = 0.0\n",
      "Iteration 49000 with error = 0.0\n",
      "Iteration 50000 with error = 0.0\n",
      "Iteration 51000 with error = 0.0\n",
      "Iteration 52000 with error = 0.0\n",
      "Iteration 53000 with error = 0.0\n",
      "Iteration 54000 with error = 0.0\n",
      "Iteration 55000 with error = 0.0\n",
      "Iteration 56000 with error = 0.0\n",
      "Iteration 57000 with error = 0.0\n",
      "Iteration 58000 with error = 0.0\n",
      "Iteration 59000 with error = 0.0\n",
      "Iteration 60000 with error = 0.0\n",
      "Iteration 61000 with error = 0.0\n",
      "Iteration 62000 with error = 0.0\n",
      "Iteration 63000 with error = 0.0\n",
      "Iteration 64000 with error = 0.0\n",
      "Iteration 65000 with error = 0.0\n",
      "Iteration 66000 with error = 0.0\n",
      "Iteration 67000 with error = 0.0\n",
      "Iteration 68000 with error = 0.0\n",
      "Iteration 69000 with error = 0.0\n",
      "Iteration 70000 with error = 0.0\n",
      "Iteration 71000 with error = 0.0\n",
      "Iteration 72000 with error = 0.0\n",
      "Iteration 73000 with error = 0.0\n",
      "Iteration 74000 with error = 0.0\n",
      "Iteration 75000 with error = 0.0\n",
      "Iteration 76000 with error = 0.0\n",
      "Iteration 77000 with error = 0.0\n",
      "Iteration 78000 with error = 0.0\n",
      "Iteration 79000 with error = 0.0\n",
      "Iteration 80000 with error = 0.0\n",
      "Iteration 81000 with error = 0.0\n",
      "Iteration 82000 with error = 0.0\n",
      "Iteration 83000 with error = 0.0\n",
      "Iteration 84000 with error = 0.0\n",
      "Iteration 85000 with error = 0.0\n",
      "Iteration 86000 with error = 0.0\n",
      "Iteration 87000 with error = 0.0\n",
      "Iteration 88000 with error = 0.0\n",
      "Iteration 89000 with error = 0.0\n",
      "Iteration 90000 with error = 0.0\n",
      "Iteration 91000 with error = 0.0\n",
      "Iteration 92000 with error = 0.0\n",
      "Iteration 93000 with error = 0.0\n",
      "Iteration 94000 with error = 0.0\n",
      "Iteration 95000 with error = 0.0\n",
      "Iteration 96000 with error = 0.0\n",
      "Iteration 97000 with error = 0.0\n",
      "Iteration 98000 with error = 0.0\n",
      "Iteration 99000 with error = 0.0\n"
     ]
    }
   ],
   "source": [
    "# XOR function (one or the other but not both)\n",
    "X = [[0,0], [0,1], [1,0], [1,1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron(learning_rate = 0.1, num_iteration = 100000)\n",
    "# Create the architecture backward\n",
    "clf.add_output_layer(num_neuron = 1)\n",
    "clf.add_hidden_layer(num_neuron = 3)\n",
    "clf.add_hidden_layer(num_neuron = 2)\n",
    "# Train the network\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 0.0, got:  0.0\n",
      "Expected 1.0, got:  1.0\n",
      "Expected 1.0, got:  1.0\n",
      "Expected 0.0, got:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected 0.0, got: \",clf.predict([0,0]))\n",
    "print(\"Expected 1.0, got: \",clf.predict([0,1]))\n",
    "print(\"Expected 1.0, got: \",clf.predict([1,0]))\n",
    "print(\"Expected 0.0, got: \",clf.predict([1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
