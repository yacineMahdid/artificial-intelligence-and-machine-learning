{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources for the perceptron\n",
    "https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975\n",
    "https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
    "\n",
    "## Content\n",
    "In this notebook we make a perceptron class from scratch that we can train with data on linear functions. The dataset that will be used is the iris dataset. There is two version of the code: \n",
    "- one that uses numpy and pandas to alleviate the setting up before doing the actual algorithmic implementation.\n",
    "- another one that doesn't use either pandas or numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# Perceptron with numpy + pandas\n",
    "# The guy is great, will definetly give a shoutout\n",
    "class Perceptron():\n",
    "    '''\n",
    "        Perceptron Learning Algorithm that can be train using a \n",
    "        fit and predict methodology with numpy\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit(self, X, y, learning_rate = 0.01, num_iteration = 100):\n",
    "        \n",
    "        (num_row, num_feature) = X.shape\n",
    "        \n",
    "        # Randomly initalize the weights\n",
    "        self.weights = np.random.rand(num_feature+1) \n",
    "\n",
    "        # Launch the training algorithm\n",
    "        for i in range(num_iteration):\n",
    "            \n",
    "            # Stochastic Gradient Descent\n",
    "            r_i = random.randint(0,num_row-1)\n",
    "            row = X[r_i,:] # take the random sample from the dataset\n",
    "            yhat = self.predict(row)\n",
    "            error = (y[r_i] - yhat) # estimate of the gradient\n",
    "            self.weights[0] = self.weights[0] + learning_rate*error*1 # first weight one is the bias\n",
    "\n",
    "            # Update all parameters after bias\n",
    "            for f_i in range(num_feature):\n",
    "                self.weights[f_i] = self.weights[f_i] + learning_rate*error*row[f_i]\n",
    "                \n",
    "            if i % 100 == 0:\n",
    "                total_error = 0\n",
    "                for r_i in range(num_row):\n",
    "                    row = X[r_i,:]\n",
    "                    yhat = self.predict(row)\n",
    "                    error = (y[r_i] - yhat)\n",
    "                    total_error = total_error + error**2\n",
    "                mean_error = total_error/num_row\n",
    "                print(f\"Iteration {i} with error = {mean_error}\")\n",
    "        \n",
    "    def predict(self, row):\n",
    "            \n",
    "        # The activation start with the bias at weights == 0\n",
    "        activation = self.weights[0]\n",
    "        \n",
    "        # We iterate over the weights and the features in the given row\n",
    "        for weight, feature in zip(self.weights[1:], row):\n",
    "            activation = activation + weight*feature\n",
    "            \n",
    "        # Heaviside Step Function Activation\n",
    "        if activation >= 0.0:\n",
    "            return 1.0\n",
    "        return 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 with error = 0.6666666666666666\n",
      "Iteration 100 with error = 0.4\n",
      "Iteration 200 with error = 0.6266666666666667\n",
      "Iteration 300 with error = 0.47333333333333333\n",
      "Iteration 400 with error = 0.6333333333333333\n",
      "Iteration 500 with error = 0.3333333333333333\n",
      "Iteration 600 with error = 0.3333333333333333\n",
      "Iteration 700 with error = 0.4\n",
      "Iteration 800 with error = 0.36666666666666664\n",
      "Iteration 900 with error = 0.42\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data sets\n",
    "df = pd.read_csv('/home/yacine/Documents/rightbrain/data/iris_dataset.csv')\n",
    "\n",
    "# Do a one hot encoding\n",
    "df = pd.get_dummies(df,prefix=['variety'])\n",
    "X = df[['sepal.length','sepal.width','petal.length','petal.width']]\n",
    "X = X.to_numpy()\n",
    "y = df['variety_Versicolor']\n",
    "y = y.to_numpy()\n",
    "\n",
    "# Shuffle the two dataset in unison\n",
    "perm = np.random.permutation(len(X))\n",
    "X = X[perm]\n",
    "y = y[perm]\n",
    "\n",
    "clf = Perceptron()\n",
    "clf.fit(X,y, num_iteration = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Perceptron():\n",
    "    '''\n",
    "        Perceptron Learning Algorithm that can be train using a \n",
    "        fit and predict methodology, without any library\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = []\n",
    "        \n",
    "    def fit(self, X, y, learning_rate = 0.01, num_iteration = 100):\n",
    "        \n",
    "        num_row = len(X)\n",
    "        num_feature = len(X[0]) # Here we assume that we have a rectangular matrix\n",
    "        \n",
    "        # Randomly initalize the weights\n",
    "        for i in range(num_feature+1):\n",
    "            self.weights.append(random.uniform(0,1))\n",
    "        \n",
    "        # Launch the training algorithm\n",
    "        \n",
    "        for i in range(num_iteration):\n",
    "            \n",
    "            # Stochastic Gradient Descent\n",
    "            r_i = random.randint(0,num_row-1)\n",
    "            row = X[r_i]\n",
    "            yhat = self.predict(row)\n",
    "            error = (y[r_i] - yhat)\n",
    "            self.weights[0] = self.weights[0] + learning_rate*error\n",
    "\n",
    "            for f_i in range(num_feature):\n",
    "                self.weights[f_i] = self.weights[f_i] + learning_rate*error*row[f_i]\n",
    "                \n",
    "            if i % 100 == 0:\n",
    "                total_error = 0\n",
    "                for r_i in range(num_row):\n",
    "                    row = X[r_i]\n",
    "                    yhat = self.predict(row)\n",
    "                    error = (y[r_i] - yhat)\n",
    "                    total_error = total_error + error**2\n",
    "                mean_error = total_error/num_row\n",
    "                print(f\"Iteration {i} with error = {mean_error}\")\n",
    "        \n",
    "    def predict(self, row):\n",
    "            \n",
    "        # The activation start with the bias at weights == 0\n",
    "        activation = self.weights[0]\n",
    "        \n",
    "        # We iterate over the weights and the features in the given row\n",
    "        for weight, feature in zip(self.weights[1:], row):\n",
    "            activation = activation + weight*feature\n",
    "            \n",
    "        if activation >= 0.0:\n",
    "            return 1.0\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 with error = 0.6666666666666666\n",
      "Iteration 100 with error = 0.34\n",
      "Iteration 200 with error = 0.35333333333333333\n",
      "Iteration 300 with error = 0.3466666666666667\n",
      "Iteration 400 with error = 0.6\n",
      "Iteration 500 with error = 0.46\n",
      "Iteration 600 with error = 0.4533333333333333\n",
      "Iteration 700 with error = 0.34\n",
      "Iteration 800 with error = 0.43333333333333335\n",
      "Iteration 900 with error = 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def permute_together(X,y):\n",
    "    '''\n",
    "        Helper function to permute (shuffle) a matrix and a vector together\n",
    "    '''\n",
    "    \n",
    "    perm_X = []\n",
    "    perm_y = []\n",
    "    while len(X) != 0 and len(y) != 0:\n",
    "        \n",
    "        perm_id = random.randint(0,len(X)-1)\n",
    "        perm_X.append(X.pop(perm_id))\n",
    "        perm_y.append(y.pop(perm_id))\n",
    "        \n",
    "    return (perm_X, perm_y)\n",
    "\n",
    "class DataFrame():\n",
    "    '''\n",
    "        Simple dataframe to mimick the pandas library\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.header = [];\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        \n",
    "    def clean_string(self,string):\n",
    "        '''\n",
    "            Dummy function to clean up the iris dataset from (\")\n",
    "        '''\n",
    "        return string.replace('\"', '')\n",
    "    \n",
    "\n",
    "    def get_encoded_labels(self, target):\n",
    "        '''\n",
    "            Encode with 1 or 0 the y vector if it match our target variable\n",
    "        '''\n",
    "        labels = []\n",
    "        for label in self.y:\n",
    "            if label == target:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "        return labels\n",
    "    \n",
    "    def read_csv(self, filename):\n",
    "        '''\n",
    "            Read the iris dataset CSV file and populate the header, the X and the y variables\n",
    "            needed for the perceptron\n",
    "        '''\n",
    "        with open(filename, newline='', encoding=\"utf-8-sig\") as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "            \n",
    "            index = -1\n",
    "            for row in csvreader:\n",
    "                \n",
    "                # We have the header\n",
    "                if index == -1:\n",
    "                    self.header = [self.clean_string(s) for s in row[0].split(',')]\n",
    "                    index += 1\n",
    "                    continue\n",
    "                \n",
    "                # Here is the data\n",
    "                x = []\n",
    "                target = None\n",
    "                data = row[0].split(',')\n",
    "                for i in range(len(data)-1):\n",
    "                    x.append(float(data[i]))\n",
    "                \n",
    "                # Last item in the csv will be the target\n",
    "                self.y.append(self.clean_string(data[len(data)-1]))\n",
    "                self.X.append(x)\n",
    "                \n",
    "                index += 1\n",
    "        \n",
    "                \n",
    "\n",
    "# Data sets\n",
    "df = DataFrame()\n",
    "df.read_csv('/home/yacine/Documents/rightbrain/data/iris_dataset.csv')\n",
    "\n",
    "X = df.X\n",
    "y = df.get_encoded_labels('Versicolor') # encoding for 0 and 1\n",
    "\n",
    "# Shuffle the two dataset in unison\n",
    "X,y = permute_together(X,y)\n",
    "\n",
    "clf = Perceptron()\n",
    "clf.fit(X,y, num_iteration = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
